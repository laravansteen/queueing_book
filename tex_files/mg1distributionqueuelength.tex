\section
[$M/G/1$ Queue Length Distribution]
{$\mathbf{M/G/1}$ Queue Length Distribution}
\label{sec:distr-queue-length}


\subsection*{Theory and Exercises}

\Opensolutionfile{hint}
\Opensolutionfile{ans}


In Section~\ref{sec:batch-arrivals} we used level-crossing arguments to find a recursive method to compute the stationary distribution $p(n)$ of the number of items in an $M^X/M/1$ queue.
Here we apply similar arguments to find $p(n)=\P{L=n}$ for the $M/G/1$ queue.
However, we cannot simply copy the derivation of the $M^X/M/1$ queue to the $M/G/1$ queue, because in the $M^X/M/1$ queue the service times of the items are exponential, hence memoryless, while in the $M/G/1$ this is not the case.


When job service times are not memoryless, hence do not restart at arrival times, we cannot choose any moment we like to apply level-crossing.
Thus, for the $M/G/1$ queue we need to focus on moments in time in which the system `restarts', which  are job departure epochs as we will see below.
All in all, the argumentation to find the recursion for $\{p(n)\}$ is quite subtle, as it uses an interplay of the PASTA property and relation~\eqref{eq:39} between $\pi(n)$, $p(n)$ and $\delta(n)$.

An important role below is played by  the number of  arrivals $Y_k$ during the service time of the $k$th job.  Since the service times of the jobs form an i.i.d. sequence of random variables, the sequence $\{Y_k\}$ is also i.i.d. Let $Y$ be the common random variable with probability mass
 $f(j) = \P{Y = j}$; write $G(j) = \P{Y_k > j}$ for the survivor function.


\begin{exercise}[\faPhoto]
 Explain that if the service time is constant and equal to $s$, then
\begin{equation}\label{eq:4}
  \P{Y_k = j\given S=s} = e^{-\lambda s}\frac{(\lambda s)^j}{j!}.
\end{equation}
\begin{hint}
If $s$ is deterministic, the number of arrivals during a fixed
    period of time with length $s$ must be Poisson distributed.
\end{hint}
  \begin{solution}
See the hint.  The period during which the arrivals occur is $s$. 
  \end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
 Explain that 
\begin{equation}\label{eq:10}
  \P{Y_k = j} = \int_0^\infty e^{-\lambda x}\frac{(\lambda x)^j}{j!}\, \d F(x),
\end{equation}
where $F$ is the distribution of the service times.
\begin{solution}
  We use a conditioning argument to arrive at this result. The
  probability that the service time is $x$ units long is written in
  various ways in the literature: $F(\d x) = \d F(x) = \P{S\in \d x}$,
  but this all means the same thing, it is only the notation that
  differs.  (As an aside, to properly define this we need measure
  theory). When $F$ has a density $f$, then
  $\d F(x) = f(x) \d x$.  Note that when $S$ is discrete, it does not
  have a density everywhere. With this,
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x}\P{S\in \d x} =
    \int_0^\infty \P{Y_k =j\given S=x} \d F(x).
    \end{equation*}
    Using the answer of the previous problem we arrive at the result.
\end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
  If $S$ is deterministic and equal to $s$, show that
  Eq.~\eqref{eq:10} reduces to Eq.~\eqref{eq:4}.  
  \begin{hint}
 $S=s$ then
    $\P{S=s}=1$, i.e., all probability mass lies at $s$. Thus, all
    arrivals must occur during $[0,s]$.
  \end{hint}
  \begin{solution}
    Let us first attack this problem from a general point of
    view. Suppose the service time $S$ can take values
    $s_1<s_2<\cdots <s_n$, and $\P{S=s_i}=\alpha_i$. Then of course we
    want that $\P{S\leq s_n} = \sum_{i=1}^n \alpha_i = 1$. The
    distribution function $F$ of $S$ is in this case a step function,
    with steps at the points $s_1, s_2, \ldots$, and step sizes
    $\alpha_1, \alpha_2, \ldots$. Thus, $F(s_i)-F(s_i-)=\alpha_i$. We
    say that such a distribution function has \emph{atoms} at the
    points $s_1, s_2, \ldots$. In this case we write
  \begin{equation*}
    \int_0^\infty g(x) \d F(x) = \sum_{i=1}^n g(s_i) \alpha_i.
  \end{equation*}
  Thus, the integral of $g$ with respect to $F$ is the sum of $g$ at
  the points at which $F$ makes a jump times the weight of $F$ at
  these points. 

  As an example, in case $S\equiv 10$ (the service time is always 10
  time units long) the distribution function $F$ makes just one jump
  at $s_1=10$ of size $\alpha_1 = F(10)-F(10-) =1$, i.e., $F$ has the
  form
    \begin{equation*}
      F(x) = 
      \begin{cases}
        0, &\quad x< 10, \\
        1, &\quad x\geq 10.
      \end{cases}
    \end{equation*}
With this, 
\begin{equation*}
  \int_0^\infty g(x) \d F(x) = \sum_{i=1}^n g(s_i) \alpha_i = g(s_1) \alpha_1 = g(10) \cdot 1 = g(10).
\end{equation*}
Thus, the integral of $g$ with respect to this distribution $F$ is
$g(10)$.  More generally, when $F$ puts all probability mass at the
single point $s$ (rather than at the point $10$), then
\begin{equation*}
  \int_0^\infty g(x) \d F(x) = g(s).
\end{equation*}

    Let us now copy the formula of the previous problem:
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x} \d F(x).
    \end{equation*}
    We see that here the integrand is the function
    $g(x) = \P{Y_k=j\given S=x}$. It is given in the question that $F$
    puts all mass on the point $s$. Therefore,
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x} \d F(x) = 
\int_0^\infty g(x) \d F(x) = g(s). \qquad \star
    \end{equation*}

    Now we also know that if the service time takes precisely $x$ time
    units, the number of arrivals is Poisson distributed. Therefore
    \begin{equation*}
    \P{Y_k =j\given S=x} = e^{-\lambda x}\frac{(\lambda x)^j}{j!}.
    \end{equation*}
Hence, 
    \begin{equation*}
    g(x) = \P{Y_k =j\given S=x} = e^{-\lambda x}\frac{(\lambda x)^j}{j!}.
    \end{equation*}


    Finally, using $(\star)$, and taking $x=s$ in the above expression of $g$, we get
    \begin{equation*}
    \P{Y_k=j} = \int_0^\infty \P{Y_k =j\given S=x} \d F(x) = 
    g(s) = e^{-\lambda s}\frac{(\lambda s)^j}{j!}.
    \end{equation*}
  \end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]
 If $S\sim \Exp(\mu)$, show that 
  \begin{equation} \label{eq:41}
f(j) = \P{Y_k = j} = \frac{\mu}{\lambda+\mu}\left(\frac{\lambda}{\lambda+\mu}\right)^j.
  \end{equation}
  \begin{hint}
Use the ideas of Exercise~\ref{ex:15} to simplify the standard integral.
  \end{hint}
  \begin{solution}
Use conditional probability to see that 
\begin{align*}
  \P{Y_n = j} 
&= \int_0^\infty e^{-\lambda x}\frac{(\lambda x)^j}{j!}\, \d F(x) = \int_0^\infty e^{-\lambda x}\frac{(\lambda x)^j}{j!} \mu e^{-\mu x}\, \d x \\
&= \frac{\mu}{j!}\lambda^j \int_0^\infty e^{-(\lambda+\mu) x}x^j\,\d x = \frac{\mu}{j!}\left(\frac{\lambda}{\lambda+\mu}\right)^j \int_0^\infty e^{-(\lambda+\mu) x}((\lambda+\mu)x)^j\,\d x \\
&= \frac{\mu}{j!}\left(\frac{\lambda}{\lambda+\mu}\right)^j \frac{j!}{\lambda+\mu}.
\end{align*}
In the last integral, we use the hint. Specifically, 
\begin{align*}
  \int_0^\infty e^{-\alpha x} (\alpha x)^j \d x 
&=  \alpha^{-1} \int_0^\infty e^{- x} x^j \d x  
=  \alpha^{-1} -e^{-x}x^j |_0^\infty + j \int_0^\infty e^{- x} x^{j-1} \d x \\
&= \alpha^{-1} j \int_0^\infty e^{- x} x^{j-1} \d x 
= \alpha^{-1} j(j-1) \int_0^\infty e^{- x} x^{j-2} \d x \\
&= \alpha^{-1} j! \int_0^\infty e^{- x} \d x 
= j!/\alpha.
\end{align*}


In hindsight, this result could have been derived in another way, in
fact by using the result of Exercise~\ref{ex:3} in which we analyzed a
merged Poisson process. Consider the Poisson process with rate
$\lambda+\mu$ that arises when the arrival and service process are
merged. The probability that an arrival corresponds to an epoch of the
merged process is $\lambda/(\lambda+\mu)$ and the probability that a
departure corresponds to an epoch of the merged process is
$\mu/(\lambda+\mu)$.  The probability that $j$ arrivals occur before a
service occurs, is the same as the probability that a geometrically
distributed random variable with success probability
$\mu/(\lambda+\mu) = 1-p$ takes the value $j$.
  \end{solution}
\end{exercise}

\begin{exercise} [\faFlask]
 If $S\sim \Exp(\mu)$, show that 
  \begin{equation}
G(j) = \sum_{k=j+1}^\infty f(k) =  \left(\frac{\lambda}{\lambda+\mu}\right)^{j+1}.
  \end{equation}
\begin{solution}
  Take $\alpha = \lambda/(\lambda+\mu)$ so that
  $f(j) = (1-\alpha) \alpha^j$.
\begin{align*}
  G(j) 
&= \sum_{k=j+1}^\infty f(k)  = (1-\alpha) \sum_{k=j+1}^\infty \alpha^k \\
& = (1-\alpha) \sum_{k=0}^\infty \alpha^{k+j+1}, \text{ by change of variable}\\
& = (1-\alpha) \sum_{k=0}^\infty \alpha^{k}\alpha^{j+1}= (1-\alpha)\alpha^{j+1} \sum_{k=0}^\infty \alpha^k \\
&= (1-\alpha)\alpha^{j+1} \frac{1}{1-\alpha} = \alpha^{j+1}.
\end{align*}
    \end{solution}
\end{exercise}

\begin{exercise}[\faPhoto] \label{ex:30} Design a suitable numerical method to evaluate   Eq.~\eqref{eq:10} for more 
  general distribution functions $F$.
\begin{hint}
Discretize time to  a grid of points, and   approximate the integral by a summation over the grid.
\end{hint}
  \begin{solution}
  A simple numerical method is as follows. Make a grid of
  size $\d x$, for some small number $\d x$, e.g. $\d x=1/100$, and write
  $f_i = \P{S\in(i\d x, (i+1)\d x]} = F((i+1)\d x) - F(i\d x)$. Then 
  \begin{equation*}
    \begin{split}
  \P{Y_k = j} 
&= \int_0^\infty e^{-\lambda x}\frac{(\lambda x)^j}{j!} \d F(x) 
\approx \sum_{i=1}^\infty e^{-\lambda i \d x}\frac{(\lambda i\d x)^j}{j!} f_i \d x.
    \end{split}
\end{equation*}

Let's try a numerical experiment. 

\begin{pyconsole}
import numpy as np

labda = 3
mu = 4
j = 5
dx = 1 / 100


def F(x):
    return 1 - np.exp(-mu * x)


def f(x):
    return F(x + dx) - F(x)


def term(i):
    res = np.exp(-labda * i * dx)
    res *= (labda * i * dx)**j / np.math.factorial(j)
    res *= f(i*dx) * dx
    return res

\end{pyconsole}

\begin{pyconsole}
print(sum(term(i) for i in range(50)))
print(sum(term(i) for i in range(500)))
print(sum(term(i) for i in range(5000)))
\end{pyconsole}

Since I don't know when to stop the integral I just try a few values;
of course stopping the integration at $x=50$ is too small, since
$50\d x=50/100=1/2$, but I include it for illustrative
purposes. Stopping at $500$ seems OK, since the results of the last
two integrals are nearly the same. This also suggests that
$\d x=1/100$ is sufficiently small. In general, however, one must take
care and try various values for $\d x$ and the integration limits.


For more complicated situations it is best to use a
numerical library to compute the above integral. These methods have been
designed to produce good and reliable results, and, typically, it is
very hard to improve these methods. Thus, let's try a real number
cruncher.

\begin{pyconsole}
from scipy.integrate import quad

def g(x):
    return np.exp(-labda*x) * (labda*x)**j/np.math.factorial(j) * f(x)

print(quad(g, 0, np.inf))
\end{pyconsole}

This is the same as our earlier answer.
\end{solution}
\end{exercise}


Let us concentrate on a down-crossing of level $n$, see
Figure~\ref{fig:mg1_2}; recall that level $n$ lies between states $n$
and $n+1$. For job $k$ to generate a down-crossing of level $n$, two events must take place: 
 job `$k-1$' must leave $n+1$ jobs behind after its service completion, and  job $k$ must leave $n$ jobs behind. Thus, 
 \begin{equation*}
   \text{Down-crossing of level $n$} \iff \1{L(D_{k-1}) = n+1}\1{L(D_k)=n} = 1.
 \end{equation*}
Let us write this in another
way. Observe that if $L(D_{k-1})=n+1$ and no other jobs arrive during
the service time $S_k$ of job $k$, i.e., when $Y_k=0$, it must also be
that job $k$ leaves $n$ jobs behind. If, however, $Y_k>0$, then
$L(D_k)\geq n+1$.  Thus, we see that
 \begin{equation*}
   \text{Down-crossing of level $n$} \iff \1{L(D_{k-1}) = n+1}\1{Y_k=0} = 1.
 \end{equation*}
% \begin{equation*}
%   \1{L(D_{k-1}) = n+1}\1{L(D_k)=n} =  \1{L(D_{k-1}) = n+1}\1{Y_k=0}.
% \end{equation*}
Consequently, the number of down-crossings of level $n$ up to time $t$ is
\begin{equation*}
  \begin{split}
  D(n+1, 0, t) 
%&= \sum_{k=1}^\infty \1{D_{k}\leq t}\1{L(D_{k})=n} \\
&= \sum_{k=1}^{D(t)}\1{L(D_{k-1})=n+1}\1{Y_k=0}.
  \end{split}
\end{equation*}

\begin{exercise}
Use a similar derivation as in~\eqref{eq:1332} to   show that 
\begin{equation*}
  \lim_{t\to\infty} \frac{  D(n+1, 0, t)}t  = \delta \delta(n+1) f(0),
\end{equation*}
where $f(0)=\P{Y=0}$.
\begin{solution}
By using the definitions and limits developed in
Sections~\ref{sec:rate-stability} and~\ref{sec:poisson-arrivals-see},
\begin{align*}
  \lim_{t\to\infty} \frac{  D(n+1, 0, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(n+1, t)}{D(t)}\frac{ D(n+1, 0, t)}{D(n+1,t)} \\
&=   \delta \delta(n+1) \lim_{t\to\infty} \frac{ D(n+1, 0, t)}{D(n+1,t)} \\
&=   \delta \delta(n+1) \P{Y=0}\\
& = \delta \delta(n+1) f(0),
\end{align*}
where the last limit follows from the independence of $Y_k$ and
$L(D_{k-1})$. 
\end{solution}
\end{exercise}

\begin{figure}[tb]
  \centering
                  
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
\node[state] (0) {$\delta(0)$}; 
\node[state] (1) [right of=0] {$\delta(1)$}; 
\node[state] (2) [right of=1] {$\delta(2)$}; 
\node[state] (3) [right of=2] {$\delta(3)$}; 
\node[state] (4) [right of=3] {$\delta(4)$}; 
%\node[state] (5) [right of=4] {$\cdots$}; 
\node (5) [above of=4] {};

\path 
(0) edge [bend left] node[above, very near start, fill=white] {$G(3)$} (5)
(0) edge [loop below] node[below, midway, fill=white] {$f(0)$} (0)
(1) edge [bend left] node[above, very near start, fill=white] {$G(3)$} (5)
(1) edge [loop below] node[below, midway, fill=white] {$f(1)$} (1)
(1) edge [bend left] node[below, midway, fill=white] {$f(0)$} (0)
(2) edge [bend left] node[above, very near start, fill=white] {$G(2)$} (5)
(2) edge [bend left] node[below, midway, fill=white] {$f(0)$} (1)
(2) edge [loop below] node[below, midway, fill=white] {$f(1)$} (2)
(3) edge [bend left] node[above, very near start, fill=white] {$G(1)$} (5)
(3) edge [loop below] node[below, midway, fill=white] {$f(1)$} (3)
(3) edge [bend left] node[below, midway, fill=white] {$f(0)$} (2)
(4) edge [bend left] node[below, near start] {$f(0)$} (3);

% \node[circ, right=of n-2] (n-1) {$n-1$}
% edge[loop below, thick]  node[midway, fill=white] {$\lambda f(0)$} (n-1); 

\draw[-, dotted, gray] (9.7,-1)--(9.7, 4.0) node[below, black] {level $3$};


\end{tikzpicture}
\caption{Level $3$ is crossed from below with rate
  $\delta\delta(0)G(3) + \delta\delta(1)G(3) + \cdots \delta \delta(3) G(1)$ and crossed
  from above with rate $\delta\delta(4) f(0)$. }
\label{fig:mg1_2}
\end{figure}

Before we deal with the up-crossing, it is important to do the next exercise.
\begin{exercise}[\faFlask]Suppose that $L(D_{k-1})>0$.
  Why is $D_k = D_{k-1} + S_{k}$?
  However, if $L(D_{k-1}) = 0$, the time between $D_{k-1}$ and $D_k$ is \emph{not} equal to $S_k$.
  Why not?
  Can you find an expression for the distribution of $D_k-D_{k-1}$ in case $L(D_{k-1})=0$?
  \begin{hint}
    You might find some inspiration in Exercise~\ref{ex:17}.

    Realize that if $L(D_{k-1})=0$, job $k-1$ leaves behind an empty system.
    Thus, before job $k$ can leave, it has to arrive.
    In other words, $D_{k-1}<A_k$.
    Since job $k$ arrives to an empty system, his service starts right away, to that the time between $A_k$ and $D_k$ is equal to the service time of job $k$.
  \end{hint}
\begin{solution}
  When $L(D_{k-1})>0$, job $k$ is already in the system when job $k-1$
  finishes its service and leaves. Thus, at the departure time
  $D_{k-1}$ of job $k-1$, the service of job $k$ can start right away
  at $D_{k-1}$. Then, $D_k=D_{k-1}+S_k$.


    When job $k-1$ leaves an empty system behind, $D_k= A_k + S_k$,
    since job $k$ sees an empty system, hence its service can start
    right away after its arrival time at time $A_k$. Since the arrival
    process is Poisson by assumption, the time to the next arrival
    after $D_{k-1}$ is exponentially distributed with rate
    $\lambda$. (Recall the memoryless property of the inter-arrival
    times.) Thus, $A_k - D_{k-1}$ has the same distribution as $X_k$,
    so that $\P{D_k - D_{k-1} \leq x} = \P{X_k + S_k\leq x}$. 
\end{solution}
\end{exercise}

For the up-crossings, assume first that $L(D_{k-1})=n>0$.  Then an up-crossing of level $n>0$  must have occurred when $L(D_k)>n$, i.e., 
 \begin{equation*}
 \1{L(D_{k-1}) = n}\1{L(D_k)>n} = 1 \implies   \text{Up-crossing of level $n$}.
 \end{equation*}
Again, we can convert this into a statement about  the number of
arrivals $Y_k$ that occurred during the service time $S_k$ of job $k$.  If $Y_k=0$, then
job $k$ must leave $n-1$ jobs behind, so no up-crossing can
happen. Next, if $Y_k=1$, then job $k$ leaves $n$ jobs behind, so
still no up-crossing occurs. In fact, level $n$ can only be  up-crossed from level $n$ if
more than one job arrives during the service of job $k$, i.e.,
\begin{equation*}
\1{L(D_{k-1})=n} \1{Y_{k}>1} = 1 \implies   \text{Up-crossing of level $n$}.
\end{equation*}
More generally, level $n$ is up-crossed from level $m$, $0<m\leq n$ whenever
\begin{equation*}
\1{L(D_{k-1})=m} \1{Y_{k}>n-m+1} \implies   \text{Up-crossing of level $n$}.
\end{equation*}
However,  if $m=0$ (think about this),
\begin{equation*}
\1{L(D_{k})>n} = \1{L(D_{k-1})=0} \1{Y_{k}>n}   \implies   \text{Up-crossing of level $n$}.
\end{equation*}

Again we define proper counting functions,  divide by $t$, and take suitable limits to find for up-crossing rate
\begin{equation}\label{eq:555}
\delta \delta(0) G(n) + \delta \sum_{m=1}^n \delta(m) G(n-m+1).
\end{equation}

\begin{exercise}
Provide the details behind the derivation of Eq.~\eqref{eq:555}.
\begin{hint}
Define for $m=1,\ldots, n$
\begin{equation*}
  D(m, n, t) = \sum_{k=1}^{D(t)}\1{L(D_{k-1})=m}\1{Y_k>n-m+1},
\end{equation*}
and 
\begin{equation*}
  D(0, n, t) = \sum_{k=1}^{D(t)}\1{L(D_{k-1})=0}\1{Y_k>n}.
\end{equation*}
Then, divide by $D(n,t)$ and $D(t)$ and take limits.
\end{hint}
\begin{solution}
With the definition of the hint, 
\begin{align*}
  \lim_{t\to\infty} \frac{  D(0, n, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(0, t)}{D(t)}\frac{ D(0, n, t)}{D(0,t)} \\
&=   \delta \delta(0) \lim_{t\to\infty} \frac{ D(0, n, t)}{D(0,t)} \\
&=   \delta \delta(0) \P{Y>n}\\
& = \delta \delta(0) G(n).
\end{align*}

\begin{align*}
  \lim_{t\to\infty} \frac{  D(m, n, t)}t 
&=   \lim_{t\to\infty}  \frac{D(t)}t \frac{D(m, t)}{D(t)}\frac{ D(m, n, t)}{D(m,t)} \\
&=   \delta \delta(m) \lim_{t\to\infty} \frac{ D(m, n, t)}{D(m,t)} \\
&=   \delta \delta(m) \P{Y>n-m+1}\\
& = \delta \delta(m) G(n-m+1).
\end{align*}
\end{solution}
\end{exercise}

Equating the down-crossing and up-crossing rates and dividing
by $\delta$ gives
\begin{equation*}
  f(0) \delta(n+1) = \delta(0) G(n) + \sum_{m=1}^{n} \delta(m) G(n+1-m).
\end{equation*}
Noting that $\pi(n) = \delta(n)$, which follows from~\eqref{eq:39} and
the fact that the $M/G/1$ queue length process has one-step
transitions, we arrive at
\begin{equation}\label{eq:72}
  f(0) \pi(n+1) = \pi(0) G(n) + \sum_{m=1}^{n} \pi(m) G(n+1-m).
\end{equation}

Clearly, we have  again obtained a recursion by which we can compute, iteratively, the
state probabilities, and follow the approach sketched below Exercise~\ref{ex:13}. 



\begin{exercise}
  Clearly, the $M/M/1$ queue is a special case of the $M/G/1$
  queue. Apply Eq.~\eqref{eq:72} to the $M/M/1$ queue to obtain $p(n)=(1-\rho)\rho^n$. 
  \begin{hint}
 Define shorthands
    such as $\alpha=\lambda/(\lambda+\mu)$, so that
    $1-\alpha = \mu/(\lambda+\mu)$, and
    $\alpha/(1-\alpha) = \lambda /\mu = \rho$.  Then, with Eq.~\eqref{eq:41}, $f(n) = \alpha^n(1-\alpha)$ and $G(n) = \alpha^{n+1}$.  (These results do not 'come for
    free'. Of course, it's just algebra, but please try to derive this
    yourself. It's good to hone your computational skills.)
  \end{hint}
    \begin{solution}
      Take $n=0$. Then $f(0) \pi(1) = (1-\alpha)\pi(1)$, and
      $\pi(0)G(0) = \pi(0)\alpha$. Thus,
      $\pi(1) = \pi(0)\alpha/(1-\alpha) = \rho \pi(0)$. 

For $n=1$,
\begin{equation*}
  \begin{split}
  (1-\alpha)  \pi(2) 
&= \pi(0)G(1) + \pi(1)G(1) = \pi(0)G(1)(1+\rho) \\
&= \pi(0)\alpha^2(1+\rho) = \pi(0)\alpha \alpha (1+\rho) = \pi(0)\alpha \rho.
  \end{split}
\end{equation*}
Dividing by $1-\alpha$, we get
\begin{equation*}
  \pi(2) = \pi(0)\rho^2.
\end{equation*}

Finally, now that we suspect that $\pi(n) = \rho^n \pi(0)$, let's fill
it in, and see whether it checks. We divide both sides by $\pi(0)$ so
that we are left with checking that
\begin{align*}
    (1-\alpha)\rho^{n+1} 
&= \alpha^{n+1} + \sum_{m=1}^n \rho^m \alpha^{n-m+2}  \\
&= \alpha^{n+1} + \alpha^{n+2}\sum_{m=1}^n (\rho/\alpha)^m  \\
&= \alpha^{n+1} + \alpha^{n+1}\rho \sum_{m=0}^{n-1} (\rho/\alpha)^m \\
&= \alpha^{n+1} + \alpha^{n+1}\rho \frac{1-(\rho/\alpha)^n}{1-\rho/\alpha}\\
&= \alpha^{n+1} - \alpha^{n+1}(1-(\rho/\alpha)^n), \quad\text{as } \rho/\alpha = 1+\rho,\\
&= \alpha^{n+1}(\rho/\alpha)^n = \alpha \rho^n.\\
\end{align*}
Since $\rho=\alpha/(1-\alpha)$ we see that the left- and right-hand sides are the same. 

Thus we get that, by using PASTA, $p(n) = \pi(n) = \rho^n \pi(0) = \rho^n (1-\rho)$; a result we
obtained earlier for the $M/M/1$ queue.
\end{solution}
\end{exercise}




\Closesolutionfile{hint}
\Closesolutionfile{ans}

\opt{solutionfiles}{
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}

%\clearpage



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../queueing_book"
%%% End:
